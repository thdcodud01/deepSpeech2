{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbFBOE8uwJ7V",
        "outputId": "5556f785-4526-44a5-8cc0-17e1724437b0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch\n",
        "%pip install torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TqGPsmzKiUY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iN54BqnSTpk"
      },
      "source": [
        "\n",
        "### Reference\n",
        "\n",
        "- Digital Signal Processing Lecture\n",
        "https://github.com/spatialaudio/digital-signal-processing-lecture\n",
        "\n",
        "- Python for Signal Processing (unipingco)\n",
        "https://github.com/unpingco/Python-for-Signal-Processing\n",
        "\n",
        "- Audio for Deep Learning (남기현님)\n",
        "https://tykimos.github.io/2019/07/04/ISS_2nd_Deep_Learning_Conference_All_Together/\n",
        "\n",
        "- 오디오 전처리 작업을 위한 연습 (박수철님)\n",
        "https://github.com/scpark20/audio-preprocessing-practice\n",
        "\n",
        "- Musical Applications of Machine Learning\n",
        "https://mac.kaist.ac.kr/~juhan/gct634/\n",
        "\n",
        "- Awesome audio study materials for Korean (최근우님)\n",
        "https://github.com/keunwoochoi/awesome-audio-study-materials-for-korean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F4hfYNb2wu9m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb06mH8jxGtr",
        "outputId": "8affdb7d-c80f-4361-ad06-dc408d93cfff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.95G/5.95G [11:31<00:00, 9.24MB/s]\n",
            "100%|██████████| 331M/331M [00:20<00:00, 17.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True)\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8JvgqogxgtA",
        "outputId": "a0699d01-73b5-47ab-f2e1-f9a0ce7fd75f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0012, -0.0012, -0.0011,  ..., -0.0021, -0.0021, -0.0022]]),\n",
              " 16000,\n",
              " 'IN BOTH THESE HIGH MYTHICAL SUBJECTS THE SURROUNDING NATURE THOUGH SUFFERING IS STILL DIGNIFIED AND BEAUTIFUL',\n",
              " 1188,\n",
              " 133604,\n",
              " 36)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "test_dataset[100]\n",
        "#소리 데이터, 샘플레이트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wlBQJ2QopRFk"
      },
      "outputs": [],
      "source": [
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "    \"\"\"\"Levenshtein distance\"는 두 시퀀스 간의 차이를 측정하기위한 문자열 메트릭입니다.\n",
        "    \"Levenshtein distanc\"는 한 단어를 다른 단어로 변경하는 데 필요한 최소 한 문자 편집 (대체, 삽입 또는 삭제) 수로 정의됩니다.\n",
        "    \"\"\"\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ru55tshPqejt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"참조 시퀀스와 가설 시퀀스 사이의 거리를 단어 수준으로 계산합니다.\n",
        "     : param reference : 참조 문장.\n",
        "     : param hypothesis : 가설 문장.\n",
        "     : param ignore_case : 대소 문자 구분 여부.\n",
        "     : param delimiter : 입력 문장의 구분자.\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d-LCRpXVqhv8"
      },
      "outputs": [],
      "source": [
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9gBZ_3-Sqm5D"
      },
      "outputs": [],
      "source": [
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Calculate word error rate (WER).\n",
        "    WER = (Sw + Dw + Iw) / Nw\n",
        "    Sw는 대체 된 단어의 수입니다.\n",
        "    Dw는 삭제 된 단어의 수입니다.\n",
        "    Iw는 삽입 된 단어의 수입니다.\n",
        "    Nw는 참조의 단어 수입니다.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "F6YUEk-2qqaw"
      },
      "outputs": [],
      "source": [
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Calculate charactor error rate (CER).\n",
        "        CER = (Sc + Dc + Ic) / Nc\n",
        "        Sc is the number of characters substituted,\n",
        "        Dc is the number of characters deleted,\n",
        "        Ic is the number of characters inserted\n",
        "        Nc is the number of characters in the reference\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOLv1UDe89em"
      },
      "source": [
        "The outputs of the network are the graphemes of each language. At each output time-step t, the RNN\n",
        "makes a prediction over characters, p(`t|x), where `t is either a character in the alphabet or the blank\n",
        "symbol. In English we have `t ∈ {a, b, c, . . . , z,space, apostrophe, blank},"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cS2jhIrBqs_G"
      },
      "outputs": [],
      "source": [
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        a 2\n",
        "        b 3\n",
        "        c 4\n",
        "        d 5\n",
        "        e 6\n",
        "        f 7\n",
        "        g 8\n",
        "        h 9\n",
        "        i 10\n",
        "        j 11\n",
        "        k 12\n",
        "        l 13\n",
        "        m 14\n",
        "        n 15\n",
        "        o 16\n",
        "        p 17\n",
        "        q 18\n",
        "        r 19\n",
        "        s 20\n",
        "        t 21\n",
        "        u 22\n",
        "        v 23\n",
        "        w 24\n",
        "        x 25\n",
        "        y 26\n",
        "        z 27\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMUWPlL8qwQg",
        "outputId": "31cf867d-64da-481d-9def-8cc6c825ba5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VrbecqsFq8qM"
      },
      "outputs": [],
      "source": [
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HueYHhOMEiM"
      },
      "source": [
        "In both cases we integrate a language model in a beam search decoding step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RZVpzOyHq9hX"
      },
      "outputs": [],
      "source": [
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQnVyQKU9PsY"
      },
      "source": [
        "We use the clipped rectifiedlinear (ReLU) function σ(x) = min{max{x, 0}, 20} as our nonlinearity.\n",
        "\n",
        "The architectures we experiment\n",
        "with consist of one or more convolutional layers, followed by one or more recurrent layers, followed\n",
        "by one or more fully connected layers.\n",
        "The hidden representation at layer l is given by h\n",
        "l with the convention that h\n",
        "0\n",
        "represents the input\n",
        "x. The bottom of the network is one or more convolutions over the time dimension of the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XXAYnQ4arD2P"
      },
      "outputs": [],
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "# The two sets of activations are summed to form the output activations for the layer h The function g(·) can be the standard recurrent operation\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LMITaH3ALmr8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "def load_dataset(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, _, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        spectrograms.append(spec)\n",
        "\n",
        "    return spectrograms\n",
        "\n",
        "def get_dataloader(data):\n",
        "    x_train = load_dataset(data, \"train\")\n",
        "    x_valid = load_dataset(data, 'valid')\n",
        "\n",
        "    mean = np.mean(x_train)\n",
        "    std = np.std(x_train)\n",
        "    x_train = (x_train - mean)/std\n",
        "    x_valid = (x_valid - mean)/std\n",
        "\n",
        "    train_set = Dataset(x_train)\n",
        "    vaild_set = Dataset(x_valid)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=4, shuffle=True, drop_last=False)\n",
        "    valid_loader = DataLoader(vaild_set, batch_size=4, shuffle=False, drop_last=False)\n",
        "\n",
        "    return train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSRFH83jetB_"
      },
      "source": [
        "이 튜토리얼에서는 \"greedy\"디코딩 방법을 사용하여 모델의 출력을 문자로 결합하여 대화 내용을 만들 수 있습니다. \"greedy\"디코더는 모델의 소프트 맥스 확률 문자 인 모델 출력을 취하며 각 시간 단계에 대해 가장 높은 확률을 가진 레이블을 선택합니다. 라벨이 빈 라벨 인 경우 최종 성적표에서 라벨이 제거됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Omm1e-e4Mu0V"
      },
      "outputs": [],
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, epoch, iter_meter):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        iter_meter.step()\n",
        "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            # The output layer L is a softmax computing a probability distribution over characters given\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def main(learning_rate=5e-4, batch_size=8, epochs=1,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 29,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=28).to(device)\n",
        "\n",
        "    iter_meter = IterMeter()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, criterion, optimizer, epoch, iter_meter)\n",
        "        test(model, device, test_loader, criterion, epoch, iter_meter)\n",
        "        # ✅ 여기에 모델 저장 코드 추가\n",
        "\n",
        "        if not os.path.exists(\"checkpoints\"):\n",
        "            os.makedirs(\"checkpoints\")\n",
        "\n",
        "        torch.save(model.state_dict(), f\"checkpoints/model_epoch_{epoch}.pth\")\n",
        "        print(f\"✔ 모델 저장됨: checkpoints/model_epoch_{epoch}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxNEtE7yP-DA",
        "outputId": "0f37a98d-fe0a-4eef-ec09-175d17ff91ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Model Parameters 23705373\n",
            "Train Epoch: 1 [0/28539 (0%)]\tLoss: 6.567441\n",
            "Train Epoch: 1 [1600/28539 (6%)]\tLoss: 2.855597\n",
            "Train Epoch: 1 [3200/28539 (11%)]\tLoss: 2.887196\n",
            "Train Epoch: 1 [4800/28539 (17%)]\tLoss: 2.865271\n",
            "Train Epoch: 1 [6400/28539 (22%)]\tLoss: 2.855502\n",
            "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 2.924374\n",
            "Train Epoch: 1 [9600/28539 (34%)]\tLoss: 2.975642\n",
            "Train Epoch: 1 [11200/28539 (39%)]\tLoss: 2.851752\n",
            "Train Epoch: 1 [12800/28539 (45%)]\tLoss: 2.883113\n",
            "Train Epoch: 1 [14400/28539 (50%)]\tLoss: 2.872491\n",
            "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 2.854091\n",
            "Train Epoch: 1 [17600/28539 (62%)]\tLoss: 2.866946\n",
            "Train Epoch: 1 [19200/28539 (67%)]\tLoss: 2.848798\n",
            "Train Epoch: 1 [20800/28539 (73%)]\tLoss: 2.845249\n",
            "Train Epoch: 1 [22400/28539 (78%)]\tLoss: 2.818542\n",
            "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 2.714976\n",
            "Train Epoch: 1 [25600/28539 (90%)]\tLoss: 2.647863\n",
            "Train Epoch: 1 [27200/28539 (95%)]\tLoss: 2.697773\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.6616, Average CER: 0.736529 Average WER: 1.3817\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_1.pth\n",
            "Train Epoch: 2 [0/28539 (0%)]\tLoss: 2.647517\n",
            "Train Epoch: 2 [1600/28539 (6%)]\tLoss: 2.669446\n",
            "Train Epoch: 2 [3200/28539 (11%)]\tLoss: 2.589803\n",
            "Train Epoch: 2 [4800/28539 (17%)]\tLoss: 2.574807\n",
            "Train Epoch: 2 [6400/28539 (22%)]\tLoss: 2.552449\n",
            "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 2.564928\n",
            "Train Epoch: 2 [9600/28539 (34%)]\tLoss: 2.509043\n",
            "Train Epoch: 2 [11200/28539 (39%)]\tLoss: 2.582351\n",
            "Train Epoch: 2 [12800/28539 (45%)]\tLoss: 2.563630\n",
            "Train Epoch: 2 [14400/28539 (50%)]\tLoss: 2.475618\n",
            "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 2.890719\n",
            "Train Epoch: 2 [17600/28539 (62%)]\tLoss: 2.841851\n",
            "Train Epoch: 2 [19200/28539 (67%)]\tLoss: 2.804219\n",
            "Train Epoch: 2 [20800/28539 (73%)]\tLoss: 2.671708\n",
            "Train Epoch: 2 [22400/28539 (78%)]\tLoss: 2.657957\n",
            "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 2.547039\n",
            "Train Epoch: 2 [25600/28539 (90%)]\tLoss: 2.523676\n",
            "Train Epoch: 2 [27200/28539 (95%)]\tLoss: 2.530722\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.5702, Average CER: 0.725616 Average WER: 1.3168\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_2.pth\n",
            "Train Epoch: 3 [0/28539 (0%)]\tLoss: 2.536146\n",
            "Train Epoch: 3 [1600/28539 (6%)]\tLoss: 2.586659\n",
            "Train Epoch: 3 [3200/28539 (11%)]\tLoss: 2.617314\n",
            "Train Epoch: 3 [4800/28539 (17%)]\tLoss: 2.619099\n",
            "Train Epoch: 3 [6400/28539 (22%)]\tLoss: 2.545927\n",
            "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 2.565675\n",
            "Train Epoch: 3 [9600/28539 (34%)]\tLoss: 2.615788\n",
            "Train Epoch: 3 [11200/28539 (39%)]\tLoss: 2.558037\n",
            "Train Epoch: 3 [12800/28539 (45%)]\tLoss: 2.599633\n",
            "Train Epoch: 3 [14400/28539 (50%)]\tLoss: 2.603523\n",
            "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 2.525511\n",
            "Train Epoch: 3 [17600/28539 (62%)]\tLoss: 2.629907\n",
            "Train Epoch: 3 [19200/28539 (67%)]\tLoss: 2.584044\n",
            "Train Epoch: 3 [20800/28539 (73%)]\tLoss: 2.541994\n",
            "Train Epoch: 3 [22400/28539 (78%)]\tLoss: 2.541458\n",
            "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 2.544896\n",
            "Train Epoch: 3 [25600/28539 (90%)]\tLoss: 2.479709\n",
            "Train Epoch: 3 [27200/28539 (95%)]\tLoss: 2.502080\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.5542, Average CER: 0.703482 Average WER: 1.4092\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_3.pth\n",
            "Train Epoch: 4 [0/28539 (0%)]\tLoss: 2.499799\n",
            "Train Epoch: 4 [1600/28539 (6%)]\tLoss: 2.538643\n",
            "Train Epoch: 4 [3200/28539 (11%)]\tLoss: 2.550700\n",
            "Train Epoch: 4 [4800/28539 (17%)]\tLoss: 2.513321\n",
            "Train Epoch: 4 [6400/28539 (22%)]\tLoss: 2.538081\n",
            "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 2.521975\n",
            "Train Epoch: 4 [9600/28539 (34%)]\tLoss: 2.471787\n",
            "Train Epoch: 4 [11200/28539 (39%)]\tLoss: 2.523147\n",
            "Train Epoch: 4 [12800/28539 (45%)]\tLoss: 2.479232\n",
            "Train Epoch: 4 [14400/28539 (50%)]\tLoss: 2.516891\n",
            "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 2.469755\n",
            "Train Epoch: 4 [17600/28539 (62%)]\tLoss: 2.446665\n",
            "Train Epoch: 4 [19200/28539 (67%)]\tLoss: 2.436893\n",
            "Train Epoch: 4 [20800/28539 (73%)]\tLoss: 2.460617\n",
            "Train Epoch: 4 [22400/28539 (78%)]\tLoss: 2.464542\n",
            "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 2.429128\n",
            "Train Epoch: 4 [25600/28539 (90%)]\tLoss: 2.368951\n",
            "Train Epoch: 4 [27200/28539 (95%)]\tLoss: 2.401557\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.3447, Average CER: 0.645555 Average WER: 1.2195\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_4.pth\n",
            "Train Epoch: 5 [0/28539 (0%)]\tLoss: 2.419813\n",
            "Train Epoch: 5 [1600/28539 (6%)]\tLoss: 2.354459\n",
            "Train Epoch: 5 [3200/28539 (11%)]\tLoss: 2.326247\n",
            "Train Epoch: 5 [4800/28539 (17%)]\tLoss: 2.319473\n",
            "Train Epoch: 5 [6400/28539 (22%)]\tLoss: 2.276359\n",
            "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 2.297866\n",
            "Train Epoch: 5 [9600/28539 (34%)]\tLoss: 2.206256\n",
            "Train Epoch: 5 [11200/28539 (39%)]\tLoss: 2.172764\n",
            "Train Epoch: 5 [12800/28539 (45%)]\tLoss: 2.158559\n",
            "Train Epoch: 5 [14400/28539 (50%)]\tLoss: 2.129844\n",
            "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 2.186921\n",
            "Train Epoch: 5 [17600/28539 (62%)]\tLoss: 2.097016\n",
            "Train Epoch: 5 [19200/28539 (67%)]\tLoss: 2.176862\n",
            "Train Epoch: 5 [20800/28539 (73%)]\tLoss: 2.110028\n",
            "Train Epoch: 5 [22400/28539 (78%)]\tLoss: 2.034592\n",
            "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 2.087478\n",
            "Train Epoch: 5 [25600/28539 (90%)]\tLoss: 2.131948\n",
            "Train Epoch: 5 [27200/28539 (95%)]\tLoss: 2.057790\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.9031, Average CER: 0.553973 Average WER: 1.0572\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_5.pth\n",
            "Train Epoch: 6 [0/28539 (0%)]\tLoss: 2.033967\n",
            "Train Epoch: 6 [1600/28539 (6%)]\tLoss: 2.017237\n",
            "Train Epoch: 6 [3200/28539 (11%)]\tLoss: 2.034416\n",
            "Train Epoch: 6 [4800/28539 (17%)]\tLoss: 2.008240\n",
            "Train Epoch: 6 [6400/28539 (22%)]\tLoss: 1.906745\n",
            "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 2.008378\n",
            "Train Epoch: 6 [9600/28539 (34%)]\tLoss: 1.880353\n",
            "Train Epoch: 6 [11200/28539 (39%)]\tLoss: 1.922250\n",
            "Train Epoch: 6 [12800/28539 (45%)]\tLoss: 1.848880\n",
            "Train Epoch: 6 [14400/28539 (50%)]\tLoss: 1.929587\n",
            "Train Epoch: 6 [16000/28539 (56%)]\tLoss: 1.785697\n",
            "Train Epoch: 6 [17600/28539 (62%)]\tLoss: 1.953486\n",
            "Train Epoch: 6 [19200/28539 (67%)]\tLoss: 1.851610\n",
            "Train Epoch: 6 [20800/28539 (73%)]\tLoss: 1.912600\n",
            "Train Epoch: 6 [22400/28539 (78%)]\tLoss: 1.788406\n",
            "Train Epoch: 6 [24000/28539 (84%)]\tLoss: 1.813341\n",
            "Train Epoch: 6 [25600/28539 (90%)]\tLoss: 1.863472\n",
            "Train Epoch: 6 [27200/28539 (95%)]\tLoss: 1.907701\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.5721, Average CER: 0.468548 Average WER: 0.9776\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_6.pth\n",
            "Train Epoch: 7 [0/28539 (0%)]\tLoss: 1.802840\n",
            "Train Epoch: 7 [1600/28539 (6%)]\tLoss: 1.688954\n",
            "Train Epoch: 7 [3200/28539 (11%)]\tLoss: 1.748421\n",
            "Train Epoch: 7 [4800/28539 (17%)]\tLoss: 1.679712\n",
            "Train Epoch: 7 [6400/28539 (22%)]\tLoss: 1.771802\n",
            "Train Epoch: 7 [8000/28539 (28%)]\tLoss: 1.719676\n",
            "Train Epoch: 7 [9600/28539 (34%)]\tLoss: 1.721142\n",
            "Train Epoch: 7 [11200/28539 (39%)]\tLoss: 1.774545\n",
            "Train Epoch: 7 [12800/28539 (45%)]\tLoss: 1.803613\n",
            "Train Epoch: 7 [14400/28539 (50%)]\tLoss: 1.800785\n",
            "Train Epoch: 7 [16000/28539 (56%)]\tLoss: 1.677623\n",
            "Train Epoch: 7 [17600/28539 (62%)]\tLoss: 1.634384\n",
            "Train Epoch: 7 [19200/28539 (67%)]\tLoss: 1.691545\n",
            "Train Epoch: 7 [20800/28539 (73%)]\tLoss: 1.756145\n",
            "Train Epoch: 7 [22400/28539 (78%)]\tLoss: 1.507418\n",
            "Train Epoch: 7 [24000/28539 (84%)]\tLoss: 1.557733\n",
            "Train Epoch: 7 [25600/28539 (90%)]\tLoss: 1.705574\n",
            "Train Epoch: 7 [27200/28539 (95%)]\tLoss: 1.555889\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.4012, Average CER: 0.425362 Average WER: 0.9103\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_7.pth\n",
            "Train Epoch: 8 [0/28539 (0%)]\tLoss: 1.578455\n",
            "Train Epoch: 8 [1600/28539 (6%)]\tLoss: 1.659632\n",
            "Train Epoch: 8 [3200/28539 (11%)]\tLoss: 1.534020\n",
            "Train Epoch: 8 [4800/28539 (17%)]\tLoss: 1.579563\n",
            "Train Epoch: 8 [6400/28539 (22%)]\tLoss: 1.572658\n",
            "Train Epoch: 8 [8000/28539 (28%)]\tLoss: 1.524526\n",
            "Train Epoch: 8 [9600/28539 (34%)]\tLoss: 1.474423\n",
            "Train Epoch: 8 [11200/28539 (39%)]\tLoss: 1.599640\n",
            "Train Epoch: 8 [12800/28539 (45%)]\tLoss: 1.492136\n",
            "Train Epoch: 8 [14400/28539 (50%)]\tLoss: 1.465965\n",
            "Train Epoch: 8 [16000/28539 (56%)]\tLoss: 1.525250\n",
            "Train Epoch: 8 [17600/28539 (62%)]\tLoss: 1.464886\n",
            "Train Epoch: 8 [19200/28539 (67%)]\tLoss: 1.622442\n",
            "Train Epoch: 8 [20800/28539 (73%)]\tLoss: 1.431906\n",
            "Train Epoch: 8 [22400/28539 (78%)]\tLoss: 1.521411\n",
            "Train Epoch: 8 [24000/28539 (84%)]\tLoss: 1.492920\n",
            "Train Epoch: 8 [25600/28539 (90%)]\tLoss: 1.596980\n",
            "Train Epoch: 8 [27200/28539 (95%)]\tLoss: 1.434386\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.2518, Average CER: 0.382246 Average WER: 0.8792\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_8.pth\n",
            "Train Epoch: 9 [0/28539 (0%)]\tLoss: 1.499687\n",
            "Train Epoch: 9 [1600/28539 (6%)]\tLoss: 1.461890\n",
            "Train Epoch: 9 [3200/28539 (11%)]\tLoss: 1.438265\n",
            "Train Epoch: 9 [4800/28539 (17%)]\tLoss: 1.388707\n",
            "Train Epoch: 9 [6400/28539 (22%)]\tLoss: 1.403535\n",
            "Train Epoch: 9 [8000/28539 (28%)]\tLoss: 1.439883\n",
            "Train Epoch: 9 [9600/28539 (34%)]\tLoss: 1.501455\n",
            "Train Epoch: 9 [11200/28539 (39%)]\tLoss: 1.490959\n",
            "Train Epoch: 9 [12800/28539 (45%)]\tLoss: 1.532575\n",
            "Train Epoch: 9 [14400/28539 (50%)]\tLoss: 1.426003\n",
            "Train Epoch: 9 [16000/28539 (56%)]\tLoss: 1.482575\n",
            "Train Epoch: 9 [17600/28539 (62%)]\tLoss: 1.450232\n",
            "Train Epoch: 9 [19200/28539 (67%)]\tLoss: 1.444248\n",
            "Train Epoch: 9 [20800/28539 (73%)]\tLoss: 1.442409\n",
            "Train Epoch: 9 [22400/28539 (78%)]\tLoss: 1.366414\n",
            "Train Epoch: 9 [24000/28539 (84%)]\tLoss: 1.395301\n",
            "Train Epoch: 9 [25600/28539 (90%)]\tLoss: 1.362310\n",
            "Train Epoch: 9 [27200/28539 (95%)]\tLoss: 1.359293\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.1734, Average CER: 0.358546 Average WER: 0.8632\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_9.pth\n",
            "Train Epoch: 10 [0/28539 (0%)]\tLoss: 1.423635\n",
            "Train Epoch: 10 [1600/28539 (6%)]\tLoss: 1.348617\n",
            "Train Epoch: 10 [3200/28539 (11%)]\tLoss: 1.387158\n",
            "Train Epoch: 10 [4800/28539 (17%)]\tLoss: 1.463051\n",
            "Train Epoch: 10 [6400/28539 (22%)]\tLoss: 1.374249\n",
            "Train Epoch: 10 [8000/28539 (28%)]\tLoss: 1.423692\n",
            "Train Epoch: 10 [9600/28539 (34%)]\tLoss: 1.380405\n",
            "Train Epoch: 10 [11200/28539 (39%)]\tLoss: 1.312461\n",
            "Train Epoch: 10 [12800/28539 (45%)]\tLoss: 1.355690\n",
            "Train Epoch: 10 [14400/28539 (50%)]\tLoss: 1.417027\n",
            "Train Epoch: 10 [16000/28539 (56%)]\tLoss: 1.209674\n",
            "Train Epoch: 10 [17600/28539 (62%)]\tLoss: 1.430085\n",
            "Train Epoch: 10 [19200/28539 (67%)]\tLoss: 1.314149\n",
            "Train Epoch: 10 [20800/28539 (73%)]\tLoss: 1.446958\n",
            "Train Epoch: 10 [22400/28539 (78%)]\tLoss: 1.189070\n",
            "Train Epoch: 10 [24000/28539 (84%)]\tLoss: 1.325876\n",
            "Train Epoch: 10 [25600/28539 (90%)]\tLoss: 1.450091\n",
            "Train Epoch: 10 [27200/28539 (95%)]\tLoss: 1.252444\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.0910, Average CER: 0.327844 Average WER: 0.8040\n",
            "\n",
            "✔ 모델 저장됨: checkpoints/model_epoch_10.pth\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J564JiLdy8i2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}